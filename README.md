# Youtube-Data
  Youtube Data Project is example how raw data, collected from Youtube via Youtube API, can be transformed not only to RDBMS tables, but to very interesting Business Intelligence charts, using very powerful tools (mainly Apache Software Foundation products) in the form of Docker containers. This project’s functionalities are here to build phases of data flow on its road from Youtube to Apache Superset Business Intelligence tool, and they are described below.<br />
  ## 1. Retrieving data from Youtube Data API and persisting to HDFS
  Using Apache NiFi tool, I retrieve the data using Youtube Data API key and using REST GET API endpoints that determine kind of data to be taken. First, I get 50 most popular videos, that is, video ids, in JSON format, in form of NiFi’s flow files. Using several kinds of NiFi processors, I make these data structure a bit simpler. Then, I extract Youtube channels ids and pass them dynamically to another REST API GET endpoint, which retrieves Youtube playlists for channel ids that I’ve got. Immediately, I persist playlists to Hadoop Distributed File System (HDFS), using dynamically channel ids, this time to build directory names, within which data about channel’s playlists will be persisted. I then repeat this manner of getting data from Youtube Data API and save them to HDFS, for another kinds of data: channels statistics, video ids per channel, videos statistics and comments per video. So, HDFS is Data lake storage for raw data. This data flow in NiFi includes 6 InvokeHTTP processors and 4 PutHDFS processors, and there are few more types of processors that make flow file structure a bit simpler.<br />
  This way, I get a bunch of directories in HDFS, named by Youtube data categories’ ids, with single file in each directory. Thanks to that, it’s prety neat and easy for Apache Spark to read these data in the next step.<br />
  ## 2. Cleansing data with Apache Spark
  With this data directory hierarchy on HDFS provided, 
Pyspark can very easily read these data, do the data cleansing (drop unwanted attributes and create the retained data’s dataframe schema), and write dataframe to Postgres RDBMS. Pyspark does this with running 4 Python scripts, for every Youtube data category each (channels, playlists, videos and comments). This way, I get 4 Postgres tables.<br />
## 3. Data enrichment
Scala Spark code reads Postgres table, and enriches the data, for all tables except for Youtube channels table. Namely, it uses user defined function which does the word count of playlist title and comment text, and counts occurrences of social networks names (such as Facebook, Instagram, Twitter, …) in videos descriptions, adds new column with metrics about that and writes enriched tables to Postgres. Also, I have parsed some things from raw data in order to be more convenient for visualizing, and enriched existing data with converted data. For example, there was parsing the strings about Youtube videos duration and converting them into integer values, as well as parsing the year and month from date of publish Youtube data.<br />
## 4. Switching data from Postgres to Superset
Apache Superset Business Intelligence tool, connects to Postgres and retrieves the data from tables mentioned above. Since there are 4 Postgres tables, each for every category of data, Superset also creates 4 dashboards, where it stores data charts. These are charts made of batch manner data. They show: channel view sums per channel country,  channel video sums per channel country, channel subscriber sums per channel country per year, channels with sums of views, channel counts by year of publish, channel counts by month of publish, playlist titles per number of videos, playlist titles per playlist title length per year, playlist counts by year and month of publish, video view sums per video title and channel title, video views per video title and channel title, video counts per category, videos per ‘Instagram’ occurrences  in video description, videos per ‘Twitter’ occurrences in video description, video counts and like sum percentages per language, video counts by year and month of publish, videos with sums of views, likes and durations, video duration sums per video category, video counts per video category in relation with language, video duration average values in relation with video category and language, comment counts per comment author channel name, comment average text lengths per comment author channel name, comment like sums per comment author channel name, comment counts by year and month of publish, comment author channel titles with relation to like counts and comment text lengths, … There are even more.<br />
## 5. Connecting Youtube Data API, Kafka, Spark, Druid and Superset
Apart of batch manner data flow, mentioned above, I have implemented real time streaming data concept. To be honest and more precise, it’s a real time streaming imitation, because there is no real real-time (neither near real-time even) data flow when using Youtube Data API.<br />
Using NiFi, I have retrieved the data from Youtube Data API and send them to Kafka producer NiFi processor. Further, there is Scala Spark code, which registers to Kafka topic mentioned in NiFi (PublishKafka processor) and it reads stream from Kafka producer, so it represents Kafka consumer. Spark then adds column with current time value to existing dataframe and sends it to another Kafka topic, which is entry point for Druid. Druid ingests the data and get them to Superset, without any changes making. At the end, Superset visualizes the data to charts.<br />
Above mentioned procedure I repeat every single day separately for Youtube channels, playlists and videos. This way, Superset automatically creates charts for every day and shows slight changes of metric values on a daily basis. Some of these charts are: channel view sums per channel country with percent changes, playlist titles and channel titles per number of videos with percent changes and video like sums per video title and channel title with percent changes.<br />
